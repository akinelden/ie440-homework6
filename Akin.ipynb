{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sympy import Symbol, lambdify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Input/training.dat', sep=' ', header=None, names=['x', 'y']);\n",
    "test_data = pd.read_csv('Input/test.dat', sep=' ', header=None, names=['x', 'y']);\n",
    "\n",
    "x_train = np.array(train_data['x'])\n",
    "y_train = np.array(train_data['y'])\n",
    "\n",
    "w0 = Symbol(\"w0\")\n",
    "w1 = Symbol(\"w1\")\n",
    "w2 = Symbol(\"w2\")\n",
    "\n",
    "func_a = np.sum(np.square(y_train - w0 - w1 * x_train))\n",
    "f_a = lambdify([[w0, w1]], func_a, \"numpy\")\n",
    "gf_a = lambdify([[w0, w1]], func_a.diff([[w0, w1]]), \"numpy\")\n",
    "grad_fa = lambda x_arr : np.array(gf_a(x_arr), 'float64').reshape(1,len(x_arr))\n",
    "\n",
    "func_b = np.sum(np.square(y_train - w0 - w1 * x_train - w2 * x_train**2))\n",
    "f_b = lambdify([[w0, w1, w2]], func_b, \"numpy\")\n",
    "gf_b = lambdify([[w0, w1, w2]], func_b.diff([[w0, w1, w2]]), \"numpy\")\n",
    "grad_fb = lambda x_arr : np.array(gf_b(x_arr), 'float64').reshape(1,len(x_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_str = lambda x_k : np.array2string(x_k.reshape(len(x_k)), precision=3, separator=',')\n",
    "\n",
    "f_str = lambda x : \"{0:.4f}\".format(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputTable:    \n",
    "    def __init__(self):\n",
    "        self.table = pd.DataFrame([],columns=['k', 'x^k', 'f(x^k)', 'd^k', 'a^k', 'x^k+1'])\n",
    "    def add_row(self, k, xk, fxk, dk, ak, xkp):\n",
    "        self.table.loc[len(self.table)] = [k, np_str(xk), f_str(fxk.item()), np_str(dk), ak, np_str(xkp)]\n",
    "    def print_latex(self):\n",
    "        print(self.table.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoidalFunc = lambda output_array : 1 / (1 + np.exp(-output_array))\n",
    "sigmoidalDeriv = lambda output_array : sigmoidalFunc(output_array) * (1 - sigmoidalFunc(output_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(patterns, hiddenLayerSize, alpha = 0.5, learningRate = 0.9, epsilon = 0.001):\n",
    "    t = 0\n",
    "    P = np.size(patterns, 0)\n",
    "    w_matrix = np.random.rand(hiddenLayerSize, np.size(patterns,1))*100 # patterns data includes y values, its column size is selected since we will add x0 to input layer\n",
    "    W_matrix = np.random.rand(1, hiddenLayerSize+1)*100 # we will add h0 to hidden layer\n",
    "    while(alpha > epsilon):\n",
    "        np.random.shuffle(patterns)\n",
    "        desiredOutputs = patterns[:,:-1].reshape(-1,1)\n",
    "        inputLayers = np.transpose(np.insert(patterns, 0, -1, axis=1)[:,:-1]) # x0 is added to all patterns and its value is -1, output values are excluded\n",
    "        hiddenLayer = np.zeros((hiddenLayerSize+1, 1)) # hiddenlayersize doesn't include h0 so it's added\n",
    "        hiddenLayer[0,:] = -1 # h0 is equal to -1\n",
    "        actualOutput = np.zeros_like(desiredOutputs)\n",
    "        for p in range(P):\n",
    "            hiddenLayer[1:] = sigmoidalFunc(w_matrix @ inputLayers[:,p].reshape(-1,1))\n",
    "            actualOutput[p] = W_matrix @ hiddenLayer\n",
    "            # since the function is linear, net output is equal to actual output\n",
    "            S_output = (1 * (desiredOutputs[p] - actualOutput[p])).reshape(-1,1)\n",
    "            S_hidden = (sigmoidalDeriv(hiddenLayer[1:]) * (np.transpose(W_matrix[:,1:]) @ S_output)).reshape(-1,1)\n",
    "            delta_W = alpha * S_output @ np.transpose(hiddenLayer)\n",
    "            W_matrix += delta_W\n",
    "            delta_w = alpha * S_hidden @ np.transpose(inputLayers[:,p].reshape(-1,1))\n",
    "            w_matrix += delta_w\n",
    "        alpha = learningRate * alpha\n",
    "        t += 1\n",
    "        actualHiddens = w_matrix @ inputLayers # h1, ..., hj\n",
    "        actualOutputMatrix = W_matrix @ np.insert(actualHiddens, 0, -1, axis=0) # o1, ..., oi\n",
    "        error = np.sum(np.square(desiredOutputs - actualOutputMatrix))\n",
    "        #print(\"Iteration {0} : error = {1}\".format(t,error))\n",
    "    actualHiddens = w_matrix @ inputLayers # h1, ..., hj\n",
    "    actualOutputMatrix = W_matrix @ np.insert(actualHiddens, 0, -1, axis=0) # o1, ..., oi\n",
    "    error = np.sum(np.square(desiredOutputs - actualOutputMatrix))\n",
    "    return w_matrix, W_matrix, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[ 8.04919429e+03, -7.98117979e+08],\n        [ 1.87193892e+05, -8.29096167e+07],\n        [ 7.49566304e+03, -2.37599797e+08]]),\n array([[-371.36383673,  -83.15681161,   -9.26186584,  -24.74274724]]),\n 1.1336030417733119e+31)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns = np.array(train_data)\n",
    "backpropagation(patterns, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[ 2.38821993e+11, -5.90337080e+13, -2.35398118e+16],\n        [ 2.38821648e+11, -5.90341186e+13, -2.35401062e+16],\n        [ 2.38821796e+11, -5.90339426e+13, -2.35399800e+16]]),\n array([[-111324.93304916,   99123.48907882,   99138.75872513,\n           99132.2131042 ]]),\n 1.2332644345039047e+59)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns2 = np.insert(np.array(train_data), 1, np.square(train_data['x']), axis=1)\n",
    "backpropagation(patterns2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageError(w_matrix, W_matrix, test_data):\n",
    "    inputLayers = np.transpose(np.insert(test_data, 0, -1, axis=1)[:,:-1]) # h1, ..., hj\n",
    "    desiredOutputs = test_data[:,:-1].reshape(-1,1)\n",
    "    actualHiddens = w_matrix @ inputLayers \n",
    "    actualOutputMatrix = W_matrix @ np.insert(actualHiddens, 0, -1, axis=0) # o1, ..., oi\n",
    "    squareResiduals = np.square(desiredOutputs - actualOutputMatrix)\n",
    "    sse = np.sum(squareResiduals)\n",
    "    mse = sse / np.size(desiredOutputs)\n",
    "    variance = np.sum(np.square(mse-squareResiduals)) / (np.size(desiredOutputs) - 1)\n",
    "    return mse, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hiddenUnit(train_data, test_data, Jq = 3, epsilon = 0.001):\n",
    "    train = np.array(train_data)\n",
    "    test = np.array(test_data)\n",
    "    q = 1\n",
    "    Et = np.infty\n",
    "    while(True):\n",
    "        patterns = np.copy(train)\n",
    "        w, W, total_error = backpropagation(patterns, Jq, epsilon=epsilon)\n",
    "        Etp, var = averageError(w, W, test)\n",
    "        print(\"{0} hidden units : MSE = {1}\".format(Jq,Etp))\n",
    "        if(Etp >= Et):\n",
    "            break\n",
    "        Jq += 1\n",
    "        q += 1\n",
    "        Et = Etp\n",
    "    return Jq-1, Et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "3 hidden units : MSE = 1.7035266648118295e+28\n4 hidden units : MSE = 2.8854480026299475e+31\n"
    },
    {
     "data": {
      "text/plain": "(3, 1.7035266648118295e+28)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddenUnit(train_data, test_data, epsilon=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}