{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sympy import Symbol, lambdify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Input/training.dat', sep=' ', header=None, names=['x', 'y']);\n",
    "test_data = pd.read_csv('Input/test.dat', sep=' ', header=None, names=['x', 'y']);\n",
    "\n",
    "x_train = np.array(train_data['x'])\n",
    "y_train = np.array(train_data['y'])\n",
    "\n",
    "w0 = Symbol(\"w0\")\n",
    "w1 = Symbol(\"w1\")\n",
    "w2 = Symbol(\"w2\")\n",
    "\n",
    "func_a = np.sum(np.square(y_train - w0 - w1 * x_train))\n",
    "f_a = lambdify([[w0, w1]], func_a, \"numpy\")\n",
    "gf_a = lambdify([[w0, w1]], func_a.diff([[w0, w1]]), \"numpy\")\n",
    "grad_fa = lambda x_arr : np.array(gf_a(x_arr), 'float64').reshape(1,len(x_arr))\n",
    "\n",
    "func_b = np.sum(np.square(y_train - w0 - w1 * x_train - w2 * x_train**2))\n",
    "f_b = lambdify([[w0, w1, w2]], func_b, \"numpy\")\n",
    "gf_b = lambdify([[w0, w1, w2]], func_b.diff([[w0, w1, w2]]), \"numpy\")\n",
    "grad_fb = lambda x_arr : np.array(gf_b(x_arr), 'float64').reshape(1,len(x_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_str = lambda x_k : np.array2string(x_k.reshape(len(x_k)), precision=3, separator=',')\n",
    "\n",
    "f_str = lambda x : \"{0:.4f}\".format(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputTable:    \n",
    "    def __init__(self):\n",
    "        self.table = pd.DataFrame([],columns=['k', 'x^k', 'f(x^k)', 'd^k', 'a^k', 'x^k+1'])\n",
    "    def add_row(self, k, xk, fxk, dk, ak, xkp):\n",
    "        self.table.loc[len(self.table)] = [k, np_str(xk), f_str(fxk.item()), np_str(dk), ak, np_str(xkp)]\n",
    "    def print_latex(self):\n",
    "        print(self.table.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoidalFunc = lambda output_array : 1 / (1 + np.exp(-output_array))\n",
    "sigmoidalDeriv = lambda hiddenlayer : hiddenlayer * (1 - hiddenlayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(patterns, hiddenLayerSize, alpha = 0.5, learningRate = 0.9, epsilon = 0.001, seed = 440):\n",
    "    np.random.seed(seed)\n",
    "    t = 0\n",
    "    P = np.size(patterns, 0)\n",
    "    w_matrix = np.random.rand(hiddenLayerSize, np.size(patterns,1))*1 # patterns data includes y values, its column size is selected since we will add x0 to input layer\n",
    "    W_matrix = np.random.rand(1, hiddenLayerSize+1)*1 # we will add h0 to hidden layer\n",
    "    while(alpha > epsilon):\n",
    "        np.random.shuffle(patterns)\n",
    "        desiredOutputs = patterns[:,-1].reshape(-1,1)\n",
    "        inputLayers = np.transpose(np.insert(patterns, 0, -1, axis=1)[:,:-1]) # x0 is added to all patterns and its value is -1, output values are excluded\n",
    "        hiddenLayer = np.zeros((hiddenLayerSize+1, 1)) # hiddenlayersize doesn't include h0 so it's added\n",
    "        hiddenLayer[0,:] = -1 # h0 is equal to -1\n",
    "        actualOutput = np.zeros_like(desiredOutputs)\n",
    "        for p in range(P):\n",
    "            hiddenLayer[1:] = sigmoidalFunc(w_matrix @ inputLayers[:,p].reshape(-1,1))\n",
    "            actualOutput[p] = W_matrix @ hiddenLayer\n",
    "            # since the function is linear, net output is equal to actual output\n",
    "            S_output = (1 * (desiredOutputs[p] - actualOutput[p])).reshape(-1,1)\n",
    "            S_hidden = (sigmoidalDeriv(hiddenLayer[1:]) * (np.transpose(W_matrix[:,1:]) @ S_output)).reshape(-1,1)\n",
    "            delta_W = alpha * S_output @ np.transpose(hiddenLayer)\n",
    "            W_matrix += delta_W\n",
    "            delta_w = alpha * S_hidden @ np.transpose(inputLayers[:,p].reshape(-1,1))\n",
    "            w_matrix += delta_w\n",
    "        alpha = learningRate * alpha\n",
    "        t += 1\n",
    "        actualHiddens = sigmoidalFunc(w_matrix @ inputLayers) # h1, ..., hj\n",
    "        actualOutputMatrix = W_matrix @ np.insert(actualHiddens, 0, -1, axis=0) # o1, ..., oi\n",
    "        error = np.sum(np.square(desiredOutputs - np.transpose(actualOutputMatrix)))\n",
    "        print(\"Iteration {0} : error = {1}\".format(t,error))\n",
    "    return w_matrix, W_matrix, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagationWithForLoops(trainingData, hiddenLayerSize, alpha = 0.5, learningRate = 0.9, epsilon = 0.001, seed = 440):\n",
    "    t = 0\n",
    "    patterns = np.copy(trainingData)\n",
    "    patterns = np.insert(patterns, 0, -1, axis=1) # x0 = -1 unit is added\n",
    "    P = np.size(patterns, 0) # pattern size\n",
    "    I = 1 # output unit size\n",
    "    K = np.size(patterns, 1) - I # input layer size\n",
    "    J = hiddenLayerSize + 1 # h0 = -1 is added\n",
    "    w_matrix = np.random.rand(J, K) # weights between input and hidden layer (we will exclude first row later since h0 is excluded)\n",
    "    W_matrix = np.random.rand(I, J) # weight between hidden and output layer\n",
    "    while(alpha >= epsilon):\n",
    "        np.random.shuffle(patterns)\n",
    "        x = np.transpose(patterns[:,:-1]).reshape(K, -1)\n",
    "        y = patterns[:,-1]\n",
    "        H = np.zeros(J)\n",
    "        H[0] = -1 # h0 is equal to -1\n",
    "        O = np.zeros_like(y)\n",
    "        for p in range(P):\n",
    "            for j in range(1,J):\n",
    "                hj = np.sum(w_matrix[j] * x[:,p])\n",
    "                H[j] = sigmoidalFunc(hj)\n",
    "            for i in range(I):\n",
    "                o = np.sum(W_matrix[i] * H)\n",
    "                O[p] = o # linear function g(x) = x\n",
    "            S_O = 0 # since there is only one output unit\n",
    "            S_H = np.zeros_like(H)\n",
    "            for i in range(I):\n",
    "                S_O = 1 * (y[p] - O[p])\n",
    "            for j in range(1,J):\n",
    "                S_H[j] = sigmoidalDeriv(H[j]) * np.sum(W_matrix[0,j] * S_O)\n",
    "            for j in range(J):\n",
    "                dWj = alpha * S_O * H[j]\n",
    "                W_matrix[0,j] += dWj\n",
    "            for k in range(K):\n",
    "                dwk = alpha * S_H * x[k,p]\n",
    "                w_matrix[:,k] += dwk\n",
    "        alpha *= learningRate\n",
    "        t += 1\n",
    "        actualHiddens = sigmoidalFunc(w_matrix @ x)\n",
    "        actualHiddens[0,:] = -1 # h1, ..., hj\n",
    "        actualOutputMatrix = W_matrix @ actualHiddens # o1, ..., oi\n",
    "        error = np.sum(np.square(y - actualOutputMatrix))\n",
    "        print(\"Iteration {0} : error = {1}\".format(t,error))\n",
    "    return w_matrix, W_matrix, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Iteration 1 : error = 545201702.3026398\nIteration 2 : error = 34216771.06600089\nIteration 3 : error = 44680880.56835221\nIteration 4 : error = 7993620.881185431\nIteration 5 : error = 12828040.587517729\nIteration 6 : error = 14513311.579675596\nIteration 7 : error = 8882293.841745708\nIteration 8 : error = 8371360.57896401\nIteration 9 : error = 11547376.83829801\nIteration 10 : error = 7996422.046439336\nIteration 11 : error = 8045942.689556799\nIteration 12 : error = 10614577.894716825\nIteration 13 : error = 10069716.217366572\nIteration 14 : error = 10188591.169210207\nIteration 15 : error = 9333172.513779834\nIteration 16 : error = 8376198.572079177\nIteration 17 : error = 8685094.810365273\nIteration 18 : error = 9096962.782771751\nIteration 19 : error = 8138489.891812035\nIteration 20 : error = 8026904.107941785\nIteration 21 : error = 8304064.817181416\nIteration 22 : error = 8168498.98972702\nIteration 23 : error = 7960955.328688899\nIteration 24 : error = 9431368.281317905\nIteration 25 : error = 8585055.128782963\nIteration 26 : error = 8335864.027145286\nIteration 27 : error = 7953883.754049973\nIteration 28 : error = 7985954.190924699\nIteration 29 : error = 8122741.817839315\nIteration 30 : error = 8306539.677987245\nIteration 31 : error = 8025824.670794589\nIteration 32 : error = 8148669.267452222\nIteration 33 : error = 7971837.964022185\nIteration 34 : error = 8024951.670330154\nIteration 35 : error = 7999186.01146228\nIteration 36 : error = 8448715.257154822\nIteration 37 : error = 7999807.591367059\nIteration 38 : error = 8167479.218281937\nIteration 39 : error = 7968210.523852359\nIteration 40 : error = 7954041.53237918\nIteration 41 : error = 8123108.015497252\nIteration 42 : error = 7981940.5320939375\nIteration 43 : error = 8161363.746711877\nIteration 44 : error = 7960304.507249124\nIteration 45 : error = 7954610.552748659\nIteration 46 : error = 7994380.37268985\nIteration 47 : error = 7953988.665183424\nIteration 48 : error = 7966165.453734587\nIteration 49 : error = 7972666.096795289\nIteration 50 : error = 7955017.124714615\nIteration 51 : error = 7958922.762556596\nIteration 52 : error = 7954852.334539121\nIteration 53 : error = 7954266.209554599\nIteration 54 : error = 7953556.69698709\nIteration 55 : error = 7953540.129239839\nIteration 56 : error = 7953567.067968516\nIteration 57 : error = 7953571.516141783\nIteration 58 : error = 7953766.208643331\nIteration 59 : error = 7953540.414327322\n"
    },
    {
     "data": {
      "text/plain": "(array([[0.80939912, 0.88248547],\n        [0.98901187, 0.52307794],\n        [0.87928188, 0.43951135],\n        [0.99135261, 0.30646859]]),\n array([[-96.51162155,  96.99315048,  97.3602759 ,  97.20723927]]),\n 7953540.414327322)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns = np.array(train_data)\n",
    "backpropagationWithForLoops(patterns, 3, seed=440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/akin/bin/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n  \"\"\"Entry point for launching an IPython kernel.\nIteration 1 : error = 8816929.259708159\nIteration 2 : error = 35416253.22628992\nIteration 3 : error = 8229024.724786728\nIteration 4 : error = 13164051.38481994\nIteration 5 : error = 15566757.510988269\nIteration 6 : error = 23564751.692394815\nIteration 7 : error = 53413296.88368054\nIteration 8 : error = 8874842.832893368\nIteration 9 : error = 16104438.438315174\nIteration 10 : error = 19138031.8672025\nIteration 11 : error = 8787954.196188327\nIteration 12 : error = 9487006.006960494\nIteration 13 : error = 8848522.9449617\nIteration 14 : error = 8041621.976536453\nIteration 15 : error = 8811230.183558227\nIteration 16 : error = 7983583.827350191\nIteration 17 : error = 8200299.000644428\nIteration 18 : error = 8601616.187121753\nIteration 19 : error = 9006039.439315354\nIteration 20 : error = 8088580.127783587\nIteration 21 : error = 8830037.52002777\nIteration 22 : error = 9067076.772440638\nIteration 23 : error = 8332115.190639583\nIteration 24 : error = 7953551.613080238\nIteration 25 : error = 8135425.774272785\nIteration 26 : error = 8020732.190837531\nIteration 27 : error = 8098359.552492526\nIteration 28 : error = 8003471.914861883\nIteration 29 : error = 7964538.080020943\nIteration 30 : error = 8017402.172059927\nIteration 31 : error = 8114208.977791673\nIteration 32 : error = 8057912.8363245055\nIteration 33 : error = 8006982.595615344\nIteration 34 : error = 7953699.985097693\nIteration 35 : error = 7953622.430792454\nIteration 36 : error = 7967588.135126663\nIteration 37 : error = 8031052.611641172\nIteration 38 : error = 7998728.2759004915\nIteration 39 : error = 7987646.749575665\nIteration 40 : error = 8021721.206016026\nIteration 41 : error = 7983345.612163168\nIteration 42 : error = 7991508.821021123\nIteration 43 : error = 7958543.5875437725\nIteration 44 : error = 7957833.414874554\nIteration 45 : error = 7953550.789499461\nIteration 46 : error = 7966646.686935366\nIteration 47 : error = 7954034.677815243\nIteration 48 : error = 7953757.23565327\nIteration 49 : error = 7954619.291296683\nIteration 50 : error = 7956950.19058742\nIteration 51 : error = 7954753.102880033\nIteration 52 : error = 7954967.705624387\nIteration 53 : error = 7954106.67874203\nIteration 54 : error = 7953854.318470058\nIteration 55 : error = 7953599.827251416\nIteration 56 : error = 7953865.975258309\nIteration 57 : error = 7953552.280365617\nIteration 58 : error = 7953737.631524678\nIteration 59 : error = 7953714.347670076\n"
    },
    {
     "data": {
      "text/plain": "(array([[  0.62495165, -21.52694768],\n        [  0.25547392,   0.39632991],\n        [  0.3773151 ,   0.99657423]]),\n array([[-128.24385088, 1238.84623456,  129.41258477,  128.96205743]]),\n 7953714.347670076)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns = np.array(train_data)\n",
    "backpropagation(patterns, 3, seed=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Iteration 1 : error = 944697069.1400571\nIteration 2 : error = 43727473.75750813\nIteration 3 : error = 159908314.5821941\nIteration 4 : error = 42677214.057957344\nIteration 5 : error = 11275987.429028483\nIteration 6 : error = 17049547.397171166\nIteration 7 : error = 25096842.549336754\nIteration 8 : error = 13718095.177282214\nIteration 9 : error = 12997109.833571704\nIteration 10 : error = 19916851.98665541\nIteration 11 : error = 10254862.747661088\nIteration 12 : error = 7958938.20386924\nIteration 13 : error = 13124474.65127949\nIteration 14 : error = 8325240.935142044\nIteration 15 : error = 11576347.127680788\nIteration 16 : error = 10075010.19525402\nIteration 17 : error = 11186638.130882198\nIteration 18 : error = 8754226.932530902\nIteration 19 : error = 8602815.323399449\nIteration 20 : error = 9473650.243996458\nIteration 21 : error = 8365002.429345769\nIteration 22 : error = 7978635.053580212\nIteration 23 : error = 8024512.188551179\nIteration 24 : error = 7975198.944123716\nIteration 25 : error = 7976236.0046211975\nIteration 26 : error = 8001621.73546558\nIteration 27 : error = 8927108.402748426\nIteration 28 : error = 8378717.042459781\nIteration 29 : error = 8999155.889053414\nIteration 30 : error = 8315447.625090154\nIteration 31 : error = 8593850.95247668\nIteration 32 : error = 8553083.186105438\nIteration 33 : error = 7962019.4399436945\nIteration 34 : error = 8320639.452014858\nIteration 35 : error = 7961011.846344332\nIteration 36 : error = 7987252.273993241\nIteration 37 : error = 7959452.718772221\nIteration 38 : error = 8047389.995525811\nIteration 39 : error = 7956689.868180375\nIteration 40 : error = 7965744.225657652\nIteration 41 : error = 7984666.973620841\nIteration 42 : error = 7955366.13452106\nIteration 43 : error = 8009718.873888639\nIteration 44 : error = 7957317.424273133\nIteration 45 : error = 7997897.820451177\nIteration 46 : error = 7957928.346587618\nIteration 47 : error = 7958549.149590551\nIteration 48 : error = 7953550.196668239\nIteration 49 : error = 7965130.085669778\nIteration 50 : error = 7953868.542725339\nIteration 51 : error = 7953640.9326881375\nIteration 52 : error = 7962439.464457133\nIteration 53 : error = 7953561.661863361\nIteration 54 : error = 7956416.654726822\nIteration 55 : error = 7953889.51272339\nIteration 56 : error = 7955319.985860188\nIteration 57 : error = 7957294.719649331\nIteration 58 : error = 7953980.000567766\nIteration 59 : error = 7953557.665973738\n"
    },
    {
     "data": {
      "text/plain": "(array([[0.98901151, 0.54954473, 0.2814473 ],\n        [0.07728957, 0.4444695 , 0.47280797],\n        [0.048522  , 0.16332445, 0.11595071]]),\n array([[-95.78239566,  97.26596939,  97.05988976,  97.40050902]]),\n 7953557.665973738)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns2 = np.insert(np.array(train_data), 1, np.square(train_data['x']), axis=1)\n",
    "backpropagation(patterns2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageError(w_matrix, W_matrix, test_data):\n",
    "    inputLayers = np.transpose(np.insert(test_data, 0, -1, axis=1)[:,:-1]) # h1, ..., hj\n",
    "    desiredOutputs = test_data[:,-1].reshape(-1,1)\n",
    "    actualHiddens = sigmoidalFunc(w_matrix @ inputLayers)\n",
    "    actualOutputMatrix = W_matrix @ np.insert(actualHiddens, 0, -1, axis=0) # o1, ..., oi\n",
    "    squareResiduals = np.square(desiredOutputs - np.transpose(actualOutputMatrix))\n",
    "    sse = np.sum(squareResiduals)\n",
    "    mse = sse / np.size(desiredOutputs)\n",
    "    variance = np.sum(np.square(mse-squareResiduals)) / (np.size(desiredOutputs) - 1)\n",
    "    return mse, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hiddenUnit(train_data, test_data, Jq = 3, epsilon = 0.001, seed = 440):\n",
    "    train = np.array(train_data)\n",
    "    test = np.array(test_data)\n",
    "    q = 1\n",
    "    Et = np.infty\n",
    "    while(True):\n",
    "        patterns = np.copy(train)\n",
    "        w, W, total_error = backpropagation(patterns, Jq, epsilon=epsilon, seed = seed)\n",
    "        Etp, var = averageError(w, W, test)\n",
    "        print(\"{0} hidden units : MSE = {1} , variance = {2}\".format(Jq,Etp,var))\n",
    "        if(Etp >= Et):\n",
    "            break\n",
    "        Jq += 1\n",
    "        q += 1\n",
    "        Et = Etp\n",
    "    return Jq-1, Et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Iteration 1 : error = 100965425.56553206\nIteration 2 : error = 33729509.58483874\nIteration 3 : error = 13921816.317282429\nIteration 4 : error = 14590979.783242617\nIteration 5 : error = 19582568.944003526\nIteration 6 : error = 8531847.50010686\nIteration 7 : error = 39518501.36311612\nIteration 8 : error = 46419589.36836452\nIteration 9 : error = 8959653.81647231\nIteration 10 : error = 20369875.821804263\nIteration 11 : error = 10501351.83867907\nIteration 12 : error = 19669562.75762891\nIteration 13 : error = 8037396.332666211\nIteration 14 : error = 9273446.201298472\nIteration 15 : error = 13515007.785977617\nIteration 16 : error = 8176503.7518314235\nIteration 17 : error = 10460791.013277885\nIteration 18 : error = 10462259.313096974\nIteration 19 : error = 8958969.593242789\nIteration 20 : error = 7985119.502770321\nIteration 21 : error = 8956957.974108052\nIteration 22 : error = 8333210.39631152\nIteration 23 : error = 8118365.326898939\nIteration 24 : error = 8094198.776738492\nIteration 25 : error = 8946226.363561196\nIteration 26 : error = 8375840.259261703\nIteration 27 : error = 8049819.599357535\nIteration 28 : error = 8630887.16026052\nIteration 29 : error = 8208744.443092472\nIteration 30 : error = 8124650.537408982\nIteration 31 : error = 8122211.698905762\nIteration 32 : error = 8207675.068275652\nIteration 33 : error = 8118859.317813284\nIteration 34 : error = 8120920.1664737705\nIteration 35 : error = 8036569.002841508\nIteration 36 : error = 7974374.54300342\nIteration 37 : error = 8192569.204193883\nIteration 38 : error = 7953585.512012507\nIteration 39 : error = 7967666.692416083\nIteration 40 : error = 7981441.758788611\nIteration 41 : error = 7975377.638031191\nIteration 42 : error = 7957045.971413638\nIteration 43 : error = 8003101.217210987\nIteration 44 : error = 7954384.790892566\nIteration 45 : error = 7957899.7328152545\nIteration 46 : error = 7953828.048374807\nIteration 47 : error = 7953538.758259228\nIteration 48 : error = 7964298.143965152\nIteration 49 : error = 7954044.674210694\nIteration 50 : error = 7954125.4479085915\nIteration 51 : error = 7954446.956139365\nIteration 52 : error = 7953606.5250849705\nIteration 53 : error = 7954774.324982619\nIteration 54 : error = 7953543.992754241\nIteration 55 : error = 7953565.171864746\nIteration 56 : error = 7955134.126587527\nIteration 57 : error = 7955439.853092527\nIteration 58 : error = 7954396.230436194\nIteration 59 : error = 7953668.437351117\n3 hidden units : MSE = 99495.20650970993 , variance = 20709921102.20424\nIteration 1 : error = 67067927371.62689\nIteration 2 : error = 194257564.3211547\nIteration 3 : error = 8006842.524558813\nIteration 4 : error = 11754573.968479527\nIteration 5 : error = 9006118.107921153\nIteration 6 : error = 13064369.55811023\nIteration 7 : error = 12812424.594313618\nIteration 8 : error = 9334424.95401613\nIteration 9 : error = 8681433.219757998\nIteration 10 : error = 28408435.96895308\nIteration 11 : error = 8010592.319365583\nIteration 12 : error = 8388921.318681972\nIteration 13 : error = 8557619.447837682\nIteration 14 : error = 12402025.854200404\nIteration 15 : error = 7992911.819849575\nIteration 16 : error = 9344098.911757769\nIteration 17 : error = 8395351.094514959\nIteration 18 : error = 7956021.508581294\nIteration 19 : error = 7979831.203504757\nIteration 20 : error = 8059498.188254745\nIteration 21 : error = 8218772.750144048\nIteration 22 : error = 8258436.562911888\nIteration 23 : error = 12584757.447745552\nIteration 24 : error = 7958921.515035839\nIteration 25 : error = 8106466.499242671\nIteration 26 : error = 8412419.934728526\nIteration 27 : error = 7977207.859370153\nIteration 28 : error = 10267013.20844124\nIteration 29 : error = 7968647.194399698\nIteration 30 : error = 8912138.809085203\nIteration 31 : error = 8573250.260228867\nIteration 32 : error = 7960458.02108251\nIteration 33 : error = 7998384.305944068\nIteration 34 : error = 8023072.8142800955\nIteration 35 : error = 8295805.69572219\nIteration 36 : error = 7955119.491448723\nIteration 37 : error = 8061362.4053558055\nIteration 38 : error = 7963713.516310149\nIteration 39 : error = 7953650.067995739\nIteration 40 : error = 8039303.165071285\n/home/akin/bin/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n  \"\"\"Entry point for launching an IPython kernel.\nIteration 41 : error = 8068916.56053227\nIteration 42 : error = 8007399.968613701\nIteration 43 : error = 7960644.266169581\nIteration 44 : error = 7954353.597771403\nIteration 45 : error = 7994053.61619789\nIteration 46 : error = 7956318.015164166\nIteration 47 : error = 7980054.700408342\nIteration 48 : error = 7953891.936627002\nIteration 49 : error = 7954852.0216357205\nIteration 50 : error = 7953550.318461847\nIteration 51 : error = 7954284.068901494\nIteration 52 : error = 7953833.444947654\nIteration 53 : error = 7954347.772534698\nIteration 54 : error = 7954716.374926176\nIteration 55 : error = 7954712.642070781\nIteration 56 : error = 7954671.271552389\nIteration 57 : error = 7955889.7860016925\nIteration 58 : error = 7956176.632574032\nIteration 59 : error = 7954605.406749359\n4 hidden units : MSE = 99833.92310023739 , variance = 21347662425.57452\n"
    },
    {
     "data": {
      "text/plain": "(3, 99495.20650970993)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddenUnit(train_data, test_data, epsilon=0.001, seed = 440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Iteration 1 : error = 944697069.1400571\nIteration 2 : error = 43727473.75750813\nIteration 3 : error = 159908314.5821941\nIteration 4 : error = 42677214.057957344\nIteration 5 : error = 11275987.429028483\nIteration 6 : error = 17049547.397171166\nIteration 7 : error = 25096842.549336754\nIteration 8 : error = 13718095.177282214\nIteration 9 : error = 12997109.833571704\nIteration 10 : error = 19916851.98665541\nIteration 11 : error = 10254862.747661088\nIteration 12 : error = 7958938.20386924\nIteration 13 : error = 13124474.65127949\nIteration 14 : error = 8325240.935142044\nIteration 15 : error = 11576347.127680788\nIteration 16 : error = 10075010.19525402\nIteration 17 : error = 11186638.130882198\nIteration 18 : error = 8754226.932530902\nIteration 19 : error = 8602815.323399449\nIteration 20 : error = 9473650.243996458\nIteration 21 : error = 8365002.429345769\nIteration 22 : error = 7978635.053580212\nIteration 23 : error = 8024512.188551179\nIteration 24 : error = 7975198.944123716\nIteration 25 : error = 7976236.0046211975\nIteration 26 : error = 8001621.73546558\nIteration 27 : error = 8927108.402748426\nIteration 28 : error = 8378717.042459781\nIteration 29 : error = 8999155.889053414\nIteration 30 : error = 8315447.625090154\nIteration 31 : error = 8593850.95247668\nIteration 32 : error = 8553083.186105438\nIteration 33 : error = 7962019.4399436945\nIteration 34 : error = 8320639.452014858\nIteration 35 : error = 7961011.846344332\nIteration 36 : error = 7987252.273993241\nIteration 37 : error = 7959452.718772221\nIteration 38 : error = 8047389.995525811\nIteration 39 : error = 7956689.868180375\nIteration 40 : error = 7965744.225657652\nIteration 41 : error = 7984666.973620841\nIteration 42 : error = 7955366.13452106\nIteration 43 : error = 8009718.873888639\nIteration 44 : error = 7957317.424273133\nIteration 45 : error = 7997897.820451177\nIteration 46 : error = 7957928.346587618\nIteration 47 : error = 7958549.149590551\nIteration 48 : error = 7953550.196668239\nIteration 49 : error = 7965130.085669778\nIteration 50 : error = 7953868.542725339\nIteration 51 : error = 7953640.9326881375\nIteration 52 : error = 7962439.464457133\nIteration 53 : error = 7953561.661863361\nIteration 54 : error = 7956416.654726822\nIteration 55 : error = 7953889.51272339\nIteration 56 : error = 7955319.985860188\nIteration 57 : error = 7957294.719649331\nIteration 58 : error = 7953980.000567766\nIteration 59 : error = 7953557.665973738\n3 hidden units : MSE = 99611.75875183403 , variance = 20935913994.502293\nIteration 1 : error = 3.650901678796787e+43\nIteration 2 : error = 8.798380481269957e+62\nIteration 3 : error = 1.2279362410651437e+65\nIteration 4 : error = 1.3072454582435906e+48\nIteration 5 : error = 1815551410.4080682\nIteration 6 : error = 40679156.52623928\nIteration 7 : error = 27945435.2843748\nIteration 8 : error = 14109657.809455022\nIteration 9 : error = 12028598.95379166\nIteration 10 : error = 15206514.692440113\nIteration 11 : error = 38363758.81969322\nIteration 12 : error = 46815371.18062116\nIteration 13 : error = 15429996.912778905\nIteration 14 : error = 24491156.011920616\nIteration 15 : error = 14880516.26782205\nIteration 16 : error = 10287787.97316989\nIteration 17 : error = 8526947.254789582\nIteration 18 : error = 8180383.266243743\nIteration 19 : error = 7955918.821801062\nIteration 20 : error = 8124544.673948908\nIteration 21 : error = 9316756.391733605\nIteration 22 : error = 8567286.745310867\nIteration 23 : error = 7964252.341984745\nIteration 24 : error = 9043858.613623137\nIteration 25 : error = 8808843.901082255\nIteration 26 : error = 8172118.516011873\nIteration 27 : error = 8068013.100408508\nIteration 28 : error = 8098524.885668379\nIteration 29 : error = 7960363.304707388\nIteration 30 : error = 10060331.71778469\nIteration 31 : error = 7955958.447159659\nIteration 32 : error = 8718555.23411203\nIteration 33 : error = 7961377.6880942695\nIteration 34 : error = 8328569.127688039\nIteration 35 : error = 8319756.227470403\nIteration 36 : error = 7954334.753906447\nIteration 37 : error = 8013305.688163003\nIteration 38 : error = 8072460.733986141\nIteration 39 : error = 7954144.493076545\nIteration 40 : error = 8062460.749017723\nIteration 41 : error = 7964624.866600196\nIteration 42 : error = 8033621.366584764\nIteration 43 : error = 7977942.657185267\nIteration 44 : error = 7954738.27689427\nIteration 45 : error = 7953632.026436118\nIteration 46 : error = 7961641.389260138\nIteration 47 : error = 7953596.586239329\nIteration 48 : error = 7958529.523397369\nIteration 49 : error = 7958250.752924131\nIteration 50 : error = 7976231.261816073\nIteration 51 : error = 7962353.631254691\nIteration 52 : error = 7954739.8033296075\nIteration 53 : error = 7953540.96848232\nIteration 54 : error = 7955189.041126073\nIteration 55 : error = 7954821.498176776\nIteration 56 : error = 7954361.769713828\nIteration 57 : error = 7953681.139108176\nIteration 58 : error = 7954211.717595373\nIteration 59 : error = 7953883.937533172\n4 hidden units : MSE = 99443.59106525224 , variance = 20607324242.407787\nIteration 1 : error = 1.5977705227749003e+68\nIteration 2 : error = 1.9647110162237958e+114\nIteration 3 : error = 2.2935392317166897e+145\nIteration 4 : error = 1.7809427717025428e+160\nIteration 5 : error = 2.835543634978616e+157\nIteration 6 : error = 8.249826512642447e+134\nIteration 7 : error = 5.25959529125125e+89\nIteration 8 : error = 2.494393664424383e+17\nIteration 9 : error = 45408054.200430974\nIteration 10 : error = 7962096.6350410115\nIteration 11 : error = 18188998.631954268\nIteration 12 : error = 11904241.488508476\nIteration 13 : error = 11669173.773895748\nIteration 14 : error = 12351409.64403929\nIteration 15 : error = 9559333.785782585\nIteration 16 : error = 7960497.800130912\nIteration 17 : error = 8207270.96158751\nIteration 18 : error = 8092315.329159765\nIteration 19 : error = 10044782.211947983\nIteration 20 : error = 9875608.57321468\nIteration 21 : error = 10574974.36835157\nIteration 22 : error = 8107775.966638911\nIteration 23 : error = 9004790.88679849\nIteration 24 : error = 8108381.443689394\nIteration 25 : error = 9302619.71334357\nIteration 26 : error = 8993739.32225064\nIteration 27 : error = 8379175.548235287\nIteration 28 : error = 8537882.52005571\nIteration 29 : error = 8135057.322506374\nIteration 30 : error = 8089840.812227824\nIteration 31 : error = 8089316.506256343\nIteration 32 : error = 9142066.505789448\nIteration 33 : error = 9168968.995485272\nIteration 34 : error = 8020367.046021322\nIteration 35 : error = 8183858.56623236\nIteration 36 : error = 8217014.301864412\nIteration 37 : error = 8027601.051478348\nIteration 38 : error = 8213212.462411956\nIteration 39 : error = 8498785.935421607\nIteration 40 : error = 8018315.475828228\nIteration 41 : error = 7966799.673635375\nIteration 42 : error = 8124561.397954738\nIteration 43 : error = 8130909.4412243785\nIteration 44 : error = 7955215.953588322\nIteration 45 : error = 8010169.208050422\nIteration 46 : error = 7982579.693502539\nIteration 47 : error = 7955211.751794628\nIteration 48 : error = 8011022.696033921\nIteration 49 : error = 7975391.164114914\nIteration 50 : error = 7954603.188808657\nIteration 51 : error = 7958889.721429398\nIteration 52 : error = 7953739.956732708\nIteration 53 : error = 7953645.752814821\nIteration 54 : error = 7955614.518262145\nIteration 55 : error = 7962206.652105534\nIteration 56 : error = 7953798.973204606\nIteration 57 : error = 7956300.269634717\nIteration 58 : error = 7957617.491285887\nIteration 59 : error = 7953630.910073409\n5 hidden units : MSE = 99651.75557384599 , variance = 21011785832.359547\n"
    },
    {
     "data": {
      "text/plain": "(4, 99443.59106525224)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_d = np.insert(np.array(train_data), 1, np.square(train_data['x']), axis=1)\n",
    "test_d = np.insert(np.array(test_data), 1, np.square(test_data['x']), axis=1)\n",
    "hiddenUnit(train_d, test_d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}