{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sympy import Symbol, lambdify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Input/training.dat', sep=' ', header=None, names=['x', 'y']);\n",
    "test_data = pd.read_csv('Input/test.dat', sep=' ', header=None, names=['x', 'y']);\n",
    "\n",
    "x_train = np.array(train_data['x'])\n",
    "y_train = np.array(train_data['y'])\n",
    "\n",
    "w0 = Symbol(\"w0\")\n",
    "w1 = Symbol(\"w1\")\n",
    "w2 = Symbol(\"w2\")\n",
    "\n",
    "func_a = np.sum(np.square(y_train - w0 - w1 * x_train))\n",
    "f_a = lambdify([[w0, w1]], func_a, \"numpy\")\n",
    "gf_a = lambdify([[w0, w1]], func_a.diff([[w0, w1]]), \"numpy\")\n",
    "grad_fa = lambda x_arr : np.array(gf_a(x_arr), 'float64').reshape(1,len(x_arr))\n",
    "\n",
    "func_b = np.sum(np.square(y_train - w0 - w1 * x_train - w2 * x_train**2))\n",
    "f_b = lambdify([[w0, w1, w2]], func_b, \"numpy\")\n",
    "gf_b = lambdify([[w0, w1, w2]], func_b.diff([[w0, w1, w2]]), \"numpy\")\n",
    "grad_fb = lambda x_arr : np.array(gf_b(x_arr), 'float64').reshape(1,len(x_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_str = lambda x_k : np.array2string(x_k.reshape(len(x_k)), precision=3, separator=',')\n",
    "\n",
    "f_str = lambda x : \"{0:.4f}\".format(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputTable:    \n",
    "    def __init__(self):\n",
    "        self.table = pd.DataFrame([],columns=['k', 'x^k', 'f(x^k)', 'd^k', 'a^k', 'x^k+1'])\n",
    "    def add_row(self, k, xk, fxk, dk, ak, xkp):\n",
    "        self.table.loc[len(self.table)] = [k, np_str(xk), f_str(fxk.item()), np_str(dk), ak, np_str(xkp)]\n",
    "    def print_latex(self):\n",
    "        print(self.table.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(3,)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns = np.array([[1,2,3],[4,5,6]])\n",
    "a = np.zeros(3)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoidalFunc = lambda output_array : 1 / (1 + np.exp(-output_array))\n",
    "sigmoidalDeriv = lambda hiddenlayer : hiddenlayer * (1 - hiddenlayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(patterns, hiddenLayerSize, alpha = 0.5, learningRate = 0.9, epsilon = 0.001, seed = np.random.randint(0,100)):\n",
    "    t = 0\n",
    "    np.random.seed(seed)\n",
    "    P = np.size(patterns, 0)\n",
    "    w_matrix = np.random.rand(hiddenLayerSize, np.size(patterns,1))*1 # patterns data includes y values, its column size is selected since we will add x0 to input layer\n",
    "    W_matrix = np.random.rand(1, hiddenLayerSize+1)*1 # we will add h0 to hidden layer\n",
    "    while(alpha > epsilon):\n",
    "        np.random.shuffle(patterns)\n",
    "        desiredOutputs = patterns[:,-1].reshape(-1,1)\n",
    "        inputLayers = np.transpose(np.insert(patterns, 0, -1, axis=1)[:,:-1]) # x0 is added to all patterns and its value is -1, output values are excluded\n",
    "        hiddenLayer = np.zeros((hiddenLayerSize+1, 1)) # hiddenlayersize doesn't include h0 so it's added\n",
    "        hiddenLayer[0,:] = -1 # h0 is equal to -1\n",
    "        actualOutput = np.zeros_like(desiredOutputs)\n",
    "        for p in range(P):\n",
    "            hiddenLayer[1:] = sigmoidalFunc(w_matrix @ inputLayers[:,p].reshape(-1,1))\n",
    "            actualOutput[p] = W_matrix @ hiddenLayer\n",
    "            # since the function is linear, net output is equal to actual output\n",
    "            S_output = (1 * (desiredOutputs[p] - actualOutput[p])).reshape(-1,1)\n",
    "            S_hidden = (sigmoidalDeriv(hiddenLayer[1:]) * (np.transpose(W_matrix[:,1:]) @ S_output)).reshape(-1,1)\n",
    "            delta_W = alpha * S_output @ np.transpose(hiddenLayer)\n",
    "            W_matrix += delta_W\n",
    "            delta_w = alpha * S_hidden @ np.transpose(inputLayers[:,p].reshape(-1,1))\n",
    "            w_matrix += delta_w\n",
    "        alpha = learningRate * alpha\n",
    "        t += 1\n",
    "        actualHiddens = sigmoidalFunc(w_matrix @ inputLayers) # h1, ..., hj\n",
    "        actualOutputMatrix = W_matrix @ np.insert(actualHiddens, 0, -1, axis=0) # o1, ..., oi\n",
    "        error = np.sum(np.square(desiredOutputs - np.transpose(actualOutputMatrix)))\n",
    "        print(\"Iteration {0} : error = {1}\".format(t,error))\n",
    "    return w_matrix, W_matrix, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagationWithForLoops(trainingData, hiddenLayerSize, alpha = 0.5, learningRate = 0.9, epsilon = 0.001, seed = np.random.randint(0,100)):\n",
    "    t = 0\n",
    "    np.random.seed(seed)\n",
    "    patterns = np.copy(trainingData)\n",
    "    patterns = np.insert(patterns, 0, -1, axis=1) # x0 = -1 unit is added\n",
    "    P = np.size(patterns, 0) # pattern size\n",
    "    I = 1 # output unit size\n",
    "    K = np.size(patterns, 1) - I # input layer size\n",
    "    J = hiddenLayerSize + 1 # h0 = -1 is added\n",
    "    w_matrix = np.random.rand(J, K) # weights between input and hidden layer (we will exclude first row later since h0 is excluded)\n",
    "    W_matrix = np.random.rand(I, J) # weight between hidden and output layer\n",
    "    while(alpha >= epsilon):\n",
    "        np.random.shuffle(patterns)\n",
    "        x = np.transpose(patterns[:,:-1]).reshape(K, -1)\n",
    "        y = patterns[:,-1]\n",
    "        H = np.zeros(J)\n",
    "        H[0] = -1 # h0 is equal to -1\n",
    "        O = np.zeros_like(y)\n",
    "        for p in range(P):\n",
    "            for j in range(1,J):\n",
    "                hj = np.sum(w_matrix[j] * x[:,p])\n",
    "                H[j] = sigmoidalFunc(hj)\n",
    "            for i in range(I):\n",
    "                o = np.sum(W_matrix[i] * H)\n",
    "                O[p] = o # linear function g(x) = x\n",
    "            S_O = 0 # since there is only one output unit\n",
    "            S_H = np.zeros_like(H)\n",
    "            for i in range(I):\n",
    "                S_O = 1 * (y[p] - O[p])\n",
    "            for j in range(1,J):\n",
    "                S_H[j] = sigmoidalDeriv(H[j]) * np.sum(W_matrix[0,j] * S_O)\n",
    "            for j in range(J):\n",
    "                dWj = alpha * S_O * H[j]\n",
    "                W_matrix[0,j] += dWj\n",
    "            for k in range(K):\n",
    "                dwk = alpha * S_H * x[k,p]\n",
    "                w_matrix[:,k] += dwk\n",
    "        alpha *= learningRate\n",
    "        t += 1\n",
    "        actualHiddens = sigmoidalFunc(w_matrix @ x)\n",
    "        actualHiddens[0,:] = -1 # h1, ..., hj\n",
    "        actualOutputMatrix = W_matrix @ actualHiddens # o1, ..., oi\n",
    "        error = np.sum(np.square(y - actualOutputMatrix))\n",
    "        print(\"Iteration {0} : error = {1}\".format(t,error))\n",
    "    return w_matrix, W_matrix, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Iteration 1 : error = 1006807423.890619\nIteration 2 : error = 23869258.3102412\nIteration 3 : error = 11200764.651787676\nIteration 4 : error = 13094540.7481575\nIteration 5 : error = 28911787.667117942\nIteration 6 : error = 12996129.268918592\nIteration 7 : error = 9567145.65491038\nIteration 8 : error = 9398798.991825499\nIteration 9 : error = 24238285.426889125\nIteration 10 : error = 11579564.048979847\nIteration 11 : error = 21321846.2599155\nIteration 12 : error = 11702963.015266364\nIteration 13 : error = 8951321.602063853\nIteration 14 : error = 11644568.46351883\nIteration 15 : error = 8363362.130914341\nIteration 16 : error = 9716429.126406835\nIteration 17 : error = 8482873.206300361\nIteration 18 : error = 7969598.6007742705\nIteration 19 : error = 11253884.425861092\nIteration 20 : error = 8160376.578740317\nIteration 21 : error = 7978362.789735451\nIteration 22 : error = 8112614.054576066\nIteration 23 : error = 8355572.70983451\nIteration 24 : error = 7972604.648193134\nIteration 25 : error = 7969760.129658739\nIteration 26 : error = 8005543.964030199\nIteration 27 : error = 8217190.240202239\nIteration 28 : error = 8016235.399591896\nIteration 29 : error = 8184260.005368518\nIteration 30 : error = 8145543.516450832\nIteration 31 : error = 7993956.728324925\nIteration 32 : error = 8028446.650386945\nIteration 33 : error = 7964788.793409176\nIteration 34 : error = 7953921.309981578\nIteration 35 : error = 7968231.845170631\nIteration 36 : error = 8222935.000154406\nIteration 37 : error = 7970781.619556009\nIteration 38 : error = 7954276.649452808\nIteration 39 : error = 8003277.045235188\nIteration 40 : error = 8044136.123826732\nIteration 41 : error = 7953756.008092563\nIteration 42 : error = 7997494.026742388\nIteration 43 : error = 7954697.963543762\nIteration 44 : error = 7974492.670412539\nIteration 45 : error = 7961794.141685955\nIteration 46 : error = 7970389.964695378\nIteration 47 : error = 7995174.867534667\nIteration 48 : error = 7956575.139208258\nIteration 49 : error = 7973532.896433499\nIteration 50 : error = 7953682.8996402845\nIteration 51 : error = 7955717.374381724\nIteration 52 : error = 7954553.186093223\nIteration 53 : error = 7956091.759497317\nIteration 54 : error = 7953840.387687608\nIteration 55 : error = 7954259.259273652\nIteration 56 : error = 7954214.808489788\nIteration 57 : error = 7953716.391811027\nIteration 58 : error = 7953564.870293716\nIteration 59 : error = 7953547.223329035\n"
    },
    {
     "data": {
      "text/plain": "(array([[0.5881308 , 0.89771373],\n        [0.89153073, 0.81583748],\n        [0.03588959, 0.69175758],\n        [0.37868094, 0.51851095]]),\n array([[-96.26897942,  97.12078111,  97.19924729,  97.64553682]]),\n 7953547.223329035)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns = np.array(train_data)\n",
    "backpropagationWithForLoops(patterns, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Iteration 1 : error = 944697069.1400571\nIteration 2 : error = 43727473.75750813\nIteration 3 : error = 159908314.5821941\nIteration 4 : error = 42677214.057957344\nIteration 5 : error = 11275987.429028483\nIteration 6 : error = 17049547.397171166\nIteration 7 : error = 25096842.549336754\nIteration 8 : error = 13718095.177282214\nIteration 9 : error = 12997109.833571704\nIteration 10 : error = 19916851.98665541\nIteration 11 : error = 10254862.747661088\nIteration 12 : error = 7958938.20386924\nIteration 13 : error = 13124474.65127949\nIteration 14 : error = 8325240.935142044\nIteration 15 : error = 11576347.127680788\nIteration 16 : error = 10075010.19525402\nIteration 17 : error = 11186638.130882198\nIteration 18 : error = 8754226.932530902\nIteration 19 : error = 8602815.323399449\nIteration 20 : error = 9473650.243996458\nIteration 21 : error = 8365002.429345769\nIteration 22 : error = 7978635.053580212\nIteration 23 : error = 8024512.188551179\nIteration 24 : error = 7975198.944123716\nIteration 25 : error = 7976236.0046211975\nIteration 26 : error = 8001621.73546558\nIteration 27 : error = 8927108.402748426\nIteration 28 : error = 8378717.042459781\nIteration 29 : error = 8999155.889053414\nIteration 30 : error = 8315447.625090154\nIteration 31 : error = 8593850.95247668\nIteration 32 : error = 8553083.186105438\nIteration 33 : error = 7962019.4399436945\nIteration 34 : error = 8320639.452014858\nIteration 35 : error = 7961011.846344332\nIteration 36 : error = 7987252.273993241\nIteration 37 : error = 7959452.718772221\nIteration 38 : error = 8047389.995525811\nIteration 39 : error = 7956689.868180375\nIteration 40 : error = 7965744.225657652\nIteration 41 : error = 7984666.973620841\nIteration 42 : error = 7955366.13452106\nIteration 43 : error = 8009718.873888639\nIteration 44 : error = 7957317.424273133\nIteration 45 : error = 7997897.820451177\nIteration 46 : error = 7957928.346587618\nIteration 47 : error = 7958549.149590551\nIteration 48 : error = 7953550.196668239\nIteration 49 : error = 7965130.085669778\nIteration 50 : error = 7953868.542725339\nIteration 51 : error = 7953640.9326881375\nIteration 52 : error = 7962439.464457133\nIteration 53 : error = 7953561.661863361\nIteration 54 : error = 7956416.654726822\nIteration 55 : error = 7953889.51272339\nIteration 56 : error = 7955319.985860188\nIteration 57 : error = 7957294.719649331\nIteration 58 : error = 7953980.000567766\nIteration 59 : error = 7953557.665973738\n"
    },
    {
     "data": {
      "text/plain": "(array([[0.98901151, 0.54954473, 0.2814473 ],\n        [0.07728957, 0.4444695 , 0.47280797],\n        [0.048522  , 0.16332445, 0.11595071]]),\n array([[-95.78239566,  97.26596939,  97.05988976,  97.40050902]]),\n 7953557.665973738)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns2 = np.insert(np.array(train_data), 1, np.square(train_data['x']), axis=1)\n",
    "backpropagation(patterns2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averageError(w_matrix, W_matrix, test_data):\n",
    "    inputLayers = np.transpose(np.insert(test_data, 0, -1, axis=1)[:,:-1]) # h1, ..., hj\n",
    "    desiredOutputs = test_data[:,-1].reshape(-1,1)\n",
    "    actualHiddens = sigmoidalFunc(w_matrix @ inputLayers)\n",
    "    actualOutputMatrix = W_matrix @ np.insert(actualHiddens, 0, -1, axis=0) # o1, ..., oi\n",
    "    squareResiduals = np.square(desiredOutputs - np.transpose(actualOutputMatrix))\n",
    "    sse = np.sum(squareResiduals)\n",
    "    mse = sse / np.size(desiredOutputs)\n",
    "    variance = np.sum(np.square(mse-squareResiduals)) / (np.size(desiredOutputs) - 1)\n",
    "    return mse, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hiddenUnit(train_data, test_data, Jq = 3, epsilon = 0.001, seed = np.random.randint(0,100)):\n",
    "    train = np.array(train_data)\n",
    "    test = np.array(test_data)\n",
    "    q = 1\n",
    "    Et = np.infty\n",
    "    while(True):\n",
    "        patterns = np.copy(train)\n",
    "        w, W, total_error = backpropagation(patterns, Jq, epsilon=epsilon)\n",
    "        Etp, var = averageError(w, W, test)\n",
    "        print(\"{0} hidden units : MSE = {1}\".format(Jq,Etp))\n",
    "        if(Etp >= Et):\n",
    "            break\n",
    "        Jq += 1\n",
    "        q += 1\n",
    "        Et = Etp\n",
    "    return Jq-1, Et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Iteration 1 : error = 1628488255.723855\nIteration 2 : error = 262377252.20313102\nIteration 3 : error = 8049430.024488673\nIteration 4 : error = 30989747.747654196\nIteration 5 : error = 12915262.003412895\nIteration 6 : error = 15631310.329285646\nIteration 7 : error = 13299944.680020344\nIteration 8 : error = 15413444.534937598\nIteration 9 : error = 8202380.856884986\nIteration 10 : error = 14992747.653127668\nIteration 11 : error = 11033716.824413612\nIteration 12 : error = 9445770.65141973\nIteration 13 : error = 12820611.805164412\nIteration 14 : error = 11682656.700652601\nIteration 15 : error = 8405368.471101215\nIteration 16 : error = 8216675.404738394\nIteration 17 : error = 8616794.902400084\nIteration 18 : error = 9581692.888374507\nIteration 19 : error = 7964761.80533867\nIteration 20 : error = 10317994.35639805\nIteration 21 : error = 9264252.029135918\nIteration 22 : error = 7998443.840230706\nIteration 23 : error = 8515195.355972568\nIteration 24 : error = 8531932.063469274\nIteration 25 : error = 8479277.823442083\nIteration 26 : error = 8640470.838149486\nIteration 27 : error = 8357068.516790201\nIteration 28 : error = 7989629.98111765\nIteration 29 : error = 8293554.739478792\nIteration 30 : error = 7971211.06930102\nIteration 31 : error = 7980345.195457053\nIteration 32 : error = 8681209.595061542\nIteration 33 : error = 8188036.696106582\nIteration 34 : error = 7954103.05776144\nIteration 35 : error = 8395658.989627076\nIteration 36 : error = 7954697.220037135\nIteration 37 : error = 7953556.70960695\nIteration 38 : error = 7969059.5669474155\nIteration 39 : error = 8144075.926085456\nIteration 40 : error = 8137460.533639875\nIteration 41 : error = 8024177.29109643\nIteration 42 : error = 7958524.457027937\nIteration 43 : error = 7955829.7806303855\nIteration 44 : error = 7981557.068137416\nIteration 45 : error = 7960399.956821747\nIteration 46 : error = 7970065.5864718035\nIteration 47 : error = 7965214.718884269\nIteration 48 : error = 7976792.3064256925\nIteration 49 : error = 7957034.996358071\nIteration 50 : error = 7957087.275972136\nIteration 51 : error = 7969145.668956379\nIteration 52 : error = 7965959.052488999\nIteration 53 : error = 7962993.785321256\nIteration 54 : error = 7955951.216870158\nIteration 55 : error = 7959574.812843257\nIteration 56 : error = 7954942.288701875\nIteration 57 : error = 7953750.4054021295\nIteration 58 : error = 7954262.336394536\nIteration 59 : error = 7953770.617871752\n3 hidden units : MSE = 99695.22913718774\nIteration 1 : error = 34672304.47702641\nIteration 2 : error = 87771854.36941892\nIteration 3 : error = 28407957.43958632\nIteration 4 : error = 22919527.379661866\nIteration 5 : error = 11578399.288469113\nIteration 6 : error = 15893525.133698866\nIteration 7 : error = 16283684.23679006\nIteration 8 : error = 22163987.513283387\nIteration 9 : error = 9744881.609266363\nIteration 10 : error = 8847202.114531197\nIteration 11 : error = 10049343.150290469\nIteration 12 : error = 11311402.133289274\nIteration 13 : error = 8016025.088493631\nIteration 14 : error = 7962163.842637889\nIteration 15 : error = 8472178.26916513\nIteration 16 : error = 8988789.907021232\nIteration 17 : error = 10634403.66695325\nIteration 18 : error = 7996427.33503333\nIteration 19 : error = 8298717.208718765\nIteration 20 : error = 8197776.107526959\nIteration 21 : error = 18508267.73420778\nIteration 22 : error = 9225069.007048663\nIteration 23 : error = 8126262.403071414\nIteration 24 : error = 8189373.571036764\nIteration 25 : error = 8672885.626144413\nIteration 26 : error = 8004170.805256704\nIteration 27 : error = 8447892.496170025\nIteration 28 : error = 7953607.460956166\nIteration 29 : error = 8262289.170321607\nIteration 30 : error = 9353973.358628117\nIteration 31 : error = 8159410.190103866\nIteration 32 : error = 8225734.6158006415\nIteration 33 : error = 8164551.962542095\nIteration 34 : error = 7986254.0474948725\nIteration 35 : error = 8097705.922426964\nIteration 36 : error = 7972983.606047877\nIteration 37 : error = 8175826.9156089155\nIteration 38 : error = 8006623.4345511105\nIteration 39 : error = 7958524.307221273\nIteration 40 : error = 7961997.207043834\n/home/akin/bin/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n  \"\"\"Entry point for launching an IPython kernel.\nIteration 41 : error = 7954974.320006764\nIteration 42 : error = 7981948.3206279855\nIteration 43 : error = 7961452.869153487\nIteration 44 : error = 7955997.567998887\nIteration 45 : error = 7973094.19709067\nIteration 46 : error = 7972233.188216534\nIteration 47 : error = 7953915.037688218\nIteration 48 : error = 7967464.994814132\nIteration 49 : error = 7963396.524341183\nIteration 50 : error = 7963830.88168286\nIteration 51 : error = 7964380.147489652\nIteration 52 : error = 7953923.630097834\nIteration 53 : error = 7954167.875057183\nIteration 54 : error = 7953899.686040995\nIteration 55 : error = 7954550.140019054\nIteration 56 : error = 7954957.764155452\nIteration 57 : error = 7954211.070686884\nIteration 58 : error = 7954013.17743026\nIteration 59 : error = 7953570.847713056\n4 hidden units : MSE = 99621.7338496295\nIteration 1 : error = 3.388050297769315e+65\nIteration 2 : error = 4.1661425396602075e+111\nIteration 3 : error = 4.8634182232050065e+142\nIteration 4 : error = 3.7764645185076527e+157\nIteration 5 : error = 6.012731064873028e+154\nIteration 6 : error = 1.7493643032142137e+132\nIteration 7 : error = 1.1152899079475853e+87\nIteration 8 : error = 528981666166414.5\nIteration 9 : error = 12325983.582341798\nIteration 10 : error = 17556915.511555918\nIteration 11 : error = 16803951.307439234\nIteration 12 : error = 11340600.868787581\nIteration 13 : error = 10237463.152788313\nIteration 14 : error = 14278178.781356677\nIteration 15 : error = 20429965.31245866\nIteration 16 : error = 11353917.68603155\nIteration 17 : error = 10601652.001259668\nIteration 18 : error = 7965726.170085574\nIteration 19 : error = 10828212.009292087\nIteration 20 : error = 10338210.72529109\nIteration 21 : error = 8112912.914719775\nIteration 22 : error = 9687103.944855196\nIteration 23 : error = 8925465.07217389\nIteration 24 : error = 10146043.391207304\nIteration 25 : error = 8063811.182582091\nIteration 26 : error = 8724933.28296295\nIteration 27 : error = 10650112.757544383\nIteration 28 : error = 8014345.948377495\nIteration 29 : error = 8139408.869453631\nIteration 30 : error = 7953865.292086351\nIteration 31 : error = 7955591.198905177\nIteration 32 : error = 7984704.574994969\nIteration 33 : error = 7971876.548375156\nIteration 34 : error = 8331199.303949578\nIteration 35 : error = 7955136.3066058\nIteration 36 : error = 8028163.118859587\nIteration 37 : error = 8567304.26258766\nIteration 38 : error = 8583430.982600437\nIteration 39 : error = 7975077.070153818\nIteration 40 : error = 7960911.914155488\nIteration 41 : error = 8793857.461751131\nIteration 42 : error = 8138462.5246308865\nIteration 43 : error = 7954277.668315776\nIteration 44 : error = 7987660.829358463\nIteration 45 : error = 8057904.566403205\nIteration 46 : error = 7968258.109585092\nIteration 47 : error = 7969271.411243446\nIteration 48 : error = 7975630.337566248\nIteration 49 : error = 7965445.757129788\nIteration 50 : error = 7957048.593266052\nIteration 51 : error = 7958796.910382954\nIteration 52 : error = 7966859.021120232\nIteration 53 : error = 7956132.977909936\nIteration 54 : error = 7959268.213838145\nIteration 55 : error = 7953885.355841356\nIteration 56 : error = 7953643.791716054\nIteration 57 : error = 7953891.679556198\nIteration 58 : error = 7953979.245489365\nIteration 59 : error = 7953793.2605579775\n5 hidden units : MSE = 99700.88538462756\n"
    },
    {
     "data": {
      "text/plain": "(4, 99621.7338496295)"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddenUnit(train_data, test_data, epsilon=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}